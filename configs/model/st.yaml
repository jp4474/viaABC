_target_: src.models.lightning_module.PreTrainLightning

net:
  _target_: src.models.MAE.model.MaskedAutoencoderViT
  img_size: 1200
  patch_size: 24      
  in_chans: 6
  # Encoder
  embed_dim: 768    
  depth: 12               # ⚠️ reduce from 24 → 24-layer ViT with 2500 patches is too heavy
  num_heads: 12
  mlp_ratio: 4.0
  norm_layer: nn.LayerNorm

  # Decoder
  decoder_embed_dim: 512
  decoder_depth: 8
  decoder_num_heads: 16

  # MAE-specific
  mask_ratio: 0.75
  norm_pix_loss: false
  kld_weight: 1.0

optimizer:
  _target_: torch.optim.AdamW
  _partial_: true
  betas: [0.9, 0.95]
  lr: 0.0004
  weight_decay: 0.05

scheduler:
  _target_: src.models.components.schedulers.WarmupCosineScheduler
  _partial_: true
  warmup_steps: 53333
  max_steps: 10   # resolved, not a reference
  min_lr: 0.000001
  max_lr: 0.0005

compile: true
